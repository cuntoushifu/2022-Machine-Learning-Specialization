{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可选实验：逻辑回归，逻辑损失\n",
    "\n",
    "在这个不计分的实验中，您将：\n",
    "- 探索为什么平方误差损失不适用于逻辑回归\n",
    "- 探索逻辑损失函数"
   ],
   "id": "1579d824176b3110"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example\n",
    "from plt_logistic_loss import soup_bowl, plt_logistic_squared_error\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ],
   "id": "91c4cbc8387b29a8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归中的平方误差？\n",
    "<img align=\"left\" src=\"./images/C1_W3_SqErrorVsLogistic.png\" style=\"width:400px; padding: 10px;\">回想一下，在**线性回归**中，我们使用了**平方误差代价函数**：\n",
    "一个变量的平方误差代价函数的方程是：\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$ \n",
    " \n",
    "其中\n",
    "  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$\n"
   ],
   "id": "3930c983e9635f82"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回想一下，平方误差代价具有一个良好的特性，即跟随代价的导数将导向最小值。"
   ],
   "id": "7fceecb87b0d0edc"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97852c78127f4107a949593bf65c928b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "soup_bowl()"
   ],
   "id": "129eddf3383c3b02"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个代价函数在线性回归中表现良好，自然也可以考虑用于逻辑回归。然而，如上面的幻灯片所指出的，$f_{wb}(x)$ 现在具有一个非线性组成部分，即S型函数：$f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b)$。让我们在前面一个实验中的示例中尝试一下平方误差代价，现在包括S型函数。\n",
    "\n",
    "这是我们的训练数据："
   ],
   "id": "8f7cad66cf370260"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5331a6363c334eae83bc7baa6722c363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\n",
    "y_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)\n",
    "plt_simple_example(x_train, y_train)"
   ],
   "id": "f3a634184151818d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们使用*平方误差代价*获取代价的曲面图：\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$ \n",
    " \n",
    "其中 \n",
    "  $$f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$$\n"
   ],
   "id": "afd76b73633e9dce"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54143f99bd1c4908a3ff3f8ad3fe0cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "plt_logistic_squared_error(x_train,y_train)\n",
    "plt.show()"
   ],
   "id": "331534f0d84a3e68"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然这产生了一个相当有趣的图表，但上面的曲面远没有线性回归中的那个“汤碗”那么平滑！\n",
    "\n",
    "逻辑回归需要一个更适合其非线性特性的代价函数。这始于损失函数。下面描述了这一点。"
   ],
   "id": "9c89a0e1e3b4df4d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Loss Function\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_a.png\"     style=\" width:250px; padding: 2px; \" >\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_b.png\"     style=\" width:250px; padding: 2px; \" >\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_c.png\"     style=\" width:250px; padding: 2px; \" > "
   ],
   "id": "a46832265fdc9fc4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归使用更适合分类任务的损失函数，其中目标值为0或1，而不是任何数字。\n",
    "\n",
    ">**定义说明：** 在本课程中，这些定义被使用：\n",
    "**损失** 是一个单个示例与其目标值之间的差异的度量，而\n",
    "**代价** 是训练集上的损失度量。\n",
    "\n",
    "这被定义为：\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ 是单个数据点的代价，其中：\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{如果 $y^{(i)}=1$}\\\\\n",
    "    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{如果 $y^{(i)}=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ 是模型的预测，而 $y^{(i)}$ 是目标值。\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$，其中函数 $g$ 是S型函数。\n",
    "\n",
    "这个损失函数的定义特点是它使用了两条分开的曲线。一条是当目标是零或 ($y=0$) 时的情况，另一条是当目标是一时 ($y=1$) 的情况。综合起来，这些曲线提供了损失函数有用的行为，即在预测与目标匹配时为零，并且随着预测与目标的不匹配而迅速增加。请考虑下面的曲线："
   ],
   "id": "7efca0b84574d001"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f536f2edfc34f498fe2423ad75c99d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_two_logistic_loss_curves()"
   ],
   "id": "6c88141981e2a948"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综合起来，这些曲线类似于平方误差损失的二次曲线。请注意，x轴是 $f_{\\mathbf{w},b}$，它是S型函数的输出。S型函数的输出严格介于0和1之间。"
   ],
   "id": "6d9096ce5de06259"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的损失函数可以重新编写为更容易实现的形式。\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "  \n",
    "这是一个相当令人生畏的方程。当考虑到 $y^{(i)}$ 只能有两个值，0 和 1 时，这个方程就不那么令人畏惧了。我们可以将方程分成两个部分来考虑：\n",
    "当 $ y^{(i)} = 0$ 时，左侧项被消除：\n",
    "$$\n",
    "\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "当 $ y^{(i)} = 1$ 时，右侧项被消除：\n",
    "$$\n",
    "\\begin{align}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "好的，有了这个新的逻辑损失函数，就可以生成一个包含所有示例损失的代价函数。这将是下一个实验的主题。现在，让我们看看上面考虑的简单示例的参数与代价曲线："
   ],
   "id": "57f447c9364a08b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "cst = plt_logistic_cost(x_train,y_train)"
   ],
   "id": "538f57d3bbee8e66"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这条曲线非常适合梯度下降！它没有高原、局部最小值或不连续性。请注意，它不像平方误差的情况下那样是一个碗。代价和代价的对数都被绘制出来，以阐明当代价很小时，曲线具有一个斜坡并继续下降的事实。提醒：您可以使用鼠标旋转上面的图表。"
   ],
   "id": "b21b83e714ef522c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 恭喜！\n",
    "您已经完成了以下任务：\n",
    "- 确定平方误差损失函数不适合分类任务\n",
    "- 开发并检查了适用于分类任务的逻辑损失函数。"
   ],
   "id": "3c610892fcb5242a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
